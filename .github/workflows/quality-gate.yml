name: QualityGate - Test Coverage & Quality Assurance

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  quality-gate:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libcairo2-dev \
          libpango1.0-dev \
          libgdk-pixbuf2.0-dev \
          libffi-dev \
          shared-mime-info
          
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-cov coverage pytest-xvfb pytest-benchmark
        pip install black ruff mypy
        
    - name: Code formatting check (Black)
      run: |
        black --check --diff src/ tests/
        
    - name: Linting check (Ruff)
      run: |
        ruff check src/ tests/
        
    - name: Type checking (MyPy)
      run: |
        mypy src/ --ignore-missing-imports --no-strict-optional
      continue-on-error: true  # Type checking is advisory
      
    - name: Run VectorForge tests
      run: |
        python -m pytest tests/test_vectorforge.py \
          --cov=src/vectorforge \
          --cov-report=xml:coverage-vectorforge.xml \
          --cov-report=term-missing \
          --verbose
      continue-on-error: true
      
    - name: Run LayoutLab tests
      run: |
        python -m pytest tests/test_overlap.py tests/test_layoutlab_extended.py \
          --cov=src/layoutlab \
          --cov-report=xml:coverage-layoutlab.xml \
          --cov-report=term-missing \
          --verbose
      continue-on-error: true
      
    - name: Run GrungeWorks tests
      run: |
        python -m pytest tests/test_noise_alignment.py tests/test_grungeworks_extended.py \
          --cov=src/grungeworks \
          --cov-report=xml:coverage-grungeworks.xml \
          --cov-report=term-missing \
          --verbose
      continue-on-error: true
      
    - name: Run integration tests
      run: |
        python -m pytest tests/test_integration_e2e.py \
          --cov=src/ \
          --cov-report=xml:coverage-integration.xml \
          --cov-report=term-missing \
          --verbose
      continue-on-error: true
      
    - name: Run schema and compliance tests
      run: |
        python -m pytest tests/test_schema.py tests/test_license_compliance.py tests/test_overlay.py \
          --verbose
      continue-on-error: true
      
    - name: Generate comprehensive coverage report
      run: |
        python tests/test_coverage_report.py
        
    - name: Performance benchmarks
      run: |
        python -m pytest tests/test_layoutlab_extended.py::TestScalabilityAndPerformance::test_placement_performance_benchmark \
          --benchmark-only \
          --benchmark-json=benchmark_results.json
      continue-on-error: true
      
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        files: coverage-vectorforge.xml,coverage-layoutlab.xml,coverage-grungeworks.xml,coverage-integration.xml
        fail_ci_if_error: false
        verbose: true
        
    - name: Upload coverage artifacts
      uses: actions/upload-artifact@v3
      with:
        name: coverage-reports
        path: |
          coverage*.xml
          coverage_report.json
          benchmark_results.json
        retention-days: 30
        
    - name: Comment coverage on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          try {
            const coverageData = JSON.parse(fs.readFileSync('coverage_report.json', 'utf8'));
            const overallCoverage = coverageData.overall.coverage_percentage;
            const target = coverageData.target;
            
            const status = overallCoverage >= target ? '✅' : '❌';
            const trend = overallCoverage >= target ? 'TARGET MET' : 'BELOW TARGET';
            
            let comment = `## ${status} QualityGate Coverage Report\n\n`;
            comment += `**Overall Coverage:** ${overallCoverage.toFixed(1)}% (Target: ${target}%)\n`;
            comment += `**Status:** ${trend}\n\n`;
            comment += `### Agent Coverage Breakdown\n`;
            
            for (const [agent, data] of Object.entries(coverageData.agents)) {
              const agentStatus = data.coverage_percentage >= target ? '✅' : '❌';
              comment += `- ${agentStatus} **${agent}**: ${data.coverage_percentage.toFixed(1)}% (${data.covered_lines}/${data.total_lines} lines)\n`;
            }
            
            comment += `\n*Report generated at ${coverageData.timestamp}*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not read coverage report:', error.message);
          }
          
    - name: Fail if coverage below threshold
      run: |
        python -c "
        import json
        import sys
        
        try:
            with open('coverage_report.json', 'r') as f:
                data = json.load(f)
            
            overall_coverage = data['overall']['coverage_percentage']
            target = data['target']
            
            print(f'Overall coverage: {overall_coverage:.1f}%')
            print(f'Target: {target}%')
            
            if overall_coverage < target:
                print(f'❌ Coverage {overall_coverage:.1f}% is below target {target}%')
                sys.exit(1)
            else:
                print(f'✅ Coverage target achieved!')
                sys.exit(0)
        except FileNotFoundError:
            print('Coverage report not found')
            sys.exit(1)
        "

  stress-test:
    runs-on: ubuntu-latest
    needs: quality-gate
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[stress-test]')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xvfb memory-profiler
        
    - name: Run stress tests
      run: |
        python -m pytest tests/test_integration_e2e.py::TestWorkflowScalability \
          --verbose \
          --maxfail=1
          
    - name: Memory usage test
      run: |
        python -m pytest tests/test_integration_e2e.py::TestWorkflowScalability::test_workflow_memory_usage \
          --verbose
          
    - name: 100-page generation test
      run: |
        python -c "
        from tests.fixtures import PerformanceFixtures
        import time
        
        print('Starting 100-page stress test...')
        start_time = time.time()
        
        pages = PerformanceFixtures.get_stress_test_pages(100)
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f'Generated {len(pages)} pages in {duration:.2f} seconds')
        print(f'Average: {duration/len(pages)*1000:.1f}ms per page')
        
        if duration > 60:  # Should complete in under 1 minute
            raise Exception(f'Stress test too slow: {duration:.2f}s')
        "

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Bandit security scan
      run: |
        pip install bandit
        bandit -r src/ -f json -o bandit-report.json || true
        
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      with:
        name: security-scan
        path: bandit-report.json
        retention-days: 30